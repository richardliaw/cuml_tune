# An unique identifier for the head node and workers of this cluster.
cluster_name: gpu-instance-p3

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers. min_workers default to 0.
min_workers: 0
max_workers: 0

# docker:
#     image:
#     container_name: ray_docker

provider:
    type: aws
    region: us-east-1
    #    availability_zone: us-west-2a
    # cache_stopped_nodes: True

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
    ssh_private_key: /Users/rliaw/Research/ec2/clustercfgs/anyscale_cfgs/anyscale.pem
    #ssh_private_key: ~/Research/ec2/clustercfgs/drl.pem

head_node:
    InstanceType: p3.8xlarge
    ImageId: latest_dlami
    KeyName: anyscale
    InstanceMarketOptions:
        MarketType: spot
    BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
              VolumeSize: 500

worker_nodes:
    InstanceType: g3.8xlarge
    #ImageId: ami-025ed45832b817a35
    KeyName: anyscale
    BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
              VolumeSize: 200


setup_commands:
    - pip install progressbar
    - >-
        tmux new -d -s rapids
        'docker pull rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04'
    - tmux new -d -s my-session 'cd cuml_tune && python downloads.py'
    - pip install ray
      # - git clone https://github.com/richardliaw/ray; cd ray; git checkout trial_executor_error  &&  python setup-dev.py --yes
    #- docker run --gpus all --rm -it -v ~/data:/data -v /home/ubuntu/cuml_tune:/rapids/notebooks/cuml_tune -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04

file_mounts:
    /home/ubuntu/cuml_tune/: /Users/rliaw/Research/ec2/cuml_tune/

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []
